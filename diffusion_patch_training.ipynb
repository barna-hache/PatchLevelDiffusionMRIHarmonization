{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core utilities and system tools\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from typing import Tuple\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# Suppress specific TorchIO loader warnings (non-critical)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*torchio.*SubjectsLoader.*\")\n",
    "\n",
    "# Memory management\n",
    "import gc\n",
    "\n",
    "# PyTorch core\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.checkpoint as cp  # activation checkpointing to save memory\n",
    "\n",
    "# Training acceleration (multi-GPU, mixed precision, etc.)\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Medical imaging augmentations\n",
    "import torchio as tio\n",
    "\n",
    "# Optimization and scheduling\n",
    "from torch.optim import Adam\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from diffusers import DDIMScheduler  # diffusion scheduler\n",
    "\n",
    "# Data loading\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolumeDataset(Dataset):\n",
    "    def __init__(self, pt_dir, train=True):\n",
    "        self.pt_dir = pt_dir\n",
    "        self.pt_files = sorted([os.path.join(pt_dir, f) for f in os.listdir(pt_dir) if f.endswith(\".pt\")])\n",
    "        self.train = train\n",
    "\n",
    "        # Spatial augmentation pool (heavy 3D transforms)\n",
    "        self.transform_list = [\n",
    "            tio.RandomAffine(scales=(0.8, 1.1), degrees=8, isotropic=True, center='image', p=1),\n",
    "            tio.RandomElasticDeformation(num_control_points=7, max_displacement=10, p=1),\n",
    "            tio.RandomFlip(axes=(0,), p=1),\n",
    "        ]\n",
    "\n",
    "        # Discrete gamma shift values for intensity augmentation\n",
    "        self.gamma_values = [-0.4, -0.3, -0.2, -0.1, \n",
    "                             0, 0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pt_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.load(self.pt_files[idx])\n",
    "        volume, ds_name = data[\"volume\"], data[\"ds_name\"]\n",
    "\n",
    "        volume = volume.unsqueeze(0)\n",
    "\n",
    "        if self.train:\n",
    "            # Random selection of spatial transforms\n",
    "            n_transforms = random.choice([1, 2])\n",
    "            chosen = random.sample(self.transform_list, n_transforms)\n",
    "            transform = tio.Compose(chosen)\n",
    "\n",
    "            # TorchIO uses Subject wrappers for volumetric transforms\n",
    "            subject = tio.Subject(vol=tio.ScalarImage(tensor=volume))\n",
    "            subject = transform(subject)\n",
    "            volume = subject['vol'].data.clone()\n",
    "\n",
    "            # Gamma augmentation applied deterministically with fixed log_gamma\n",
    "            gamma_val = random.choice(self.gamma_values)\n",
    "            gamma_transform = tio.RandomGamma(log_gamma=(gamma_val, gamma_val), p=1.0)\n",
    "            subject = tio.Subject(vol=tio.ScalarImage(tensor=volume))\n",
    "            subject = gamma_transform(subject)\n",
    "            volume = subject['vol'].data.clone()\n",
    "\n",
    "            # Dataset label adjusted to record intensity transform\n",
    "            ds_name = f\"{ds_name}_gamma_{gamma_val}\"\n",
    "\n",
    "        if not self.train:\n",
    "            ds_name = f\"{ds_name}_gamma_0\"\n",
    "\n",
    "        # Z-score normalization (global over the full volume)\n",
    "        v_mean = volume.mean()\n",
    "        v_std = volume.std(unbiased=False)\n",
    "        volume_z_score = (volume - v_mean) / v_std\n",
    "\n",
    "        return volume_z_score, ds_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset and loader\n",
    "train_dataset = VolumeDataset(\"/NAS/coolio/Barnabe/CODES/diffusion_classifier_guidance/iguane_pt_train_dataset\", train=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=5, pin_memory=True, persistent_workers=False, prefetch_factor=3)\n",
    "\n",
    "# Test dataset and loader (no augmentation)\n",
    "test_dataset = VolumeDataset(\"/NAS/coolio/Barnabe/CODES/diffusion_classifier_guidance/iguane_pt_test_dataset\", train=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=5, shuffle=False, num_workers=5, pin_memory=True, persistent_workers=False, prefetch_factor=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_values = [-0.4, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "# Collect original dataset names (before augmentation)\n",
    "original_ds_names = set()\n",
    "for pt_file in tqdm(test_dataset.pt_files):\n",
    "    data = torch.load(pt_file)\n",
    "    original_ds_names.add(data[\"ds_name\"])\n",
    "\n",
    "# Generate all possible augmented labels (name + gamma)\n",
    "all_labels = set()\n",
    "for name in tqdm(original_ds_names):\n",
    "    for gamma in gamma_values:\n",
    "        all_labels.add(f\"{name}_gamma_{gamma}\")\n",
    "\n",
    "# Create bidirectional mapping between string label and integer ID\n",
    "ds2id = {name: idx + 1 for idx, name in enumerate(sorted(all_labels))}\n",
    "id2ds = {idx: name for name, idx in ds2id.items()}\n",
    "\n",
    "n_classes = len(all_labels)\n",
    "print(f\"Number of classes: {n_classes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomical representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gaussian_kernel_3d(sigma: float, device: torch.device, dtype=torch.float32):\n",
    "    # Build separable 3D Gaussian kernel (or identity if sigma=0)\n",
    "    if sigma <= 0:\n",
    "        k = torch.zeros((1,1,1,1,1), device=device, dtype=dtype)\n",
    "        k[0,0,0,0,0] = 1.0\n",
    "        return k\n",
    "    radius = int(max(1, torch.ceil(3 * torch.tensor(sigma)).item()))\n",
    "    coords = torch.arange(-radius, radius+1, device=device, dtype=dtype)\n",
    "    g1d = torch.exp(-(coords**2) / (2 * sigma**2))\n",
    "    g1d = g1d / g1d.sum()\n",
    "    g3d = g1d[:,None,None] * g1d[None,:,None] * g1d[None,None,:]\n",
    "    return g3d.unsqueeze(0).unsqueeze(0)  # (1,1,D,H,W)\n",
    "\n",
    "def _pad_for_conv(kernel):\n",
    "    # Compute symmetric padding for \"same\" 3D convolution\n",
    "    kD, kH, kW = kernel.shape[-3:]\n",
    "    return (kW//2, kW//2, kH//2, kH//2, kD//2, kD//2)\n",
    "\n",
    "def _conv3d_same(x, kernel):\n",
    "    # Apply 3D conv with replicate-padding to preserve spatial size\n",
    "    pad = _pad_for_conv(kernel)\n",
    "    x_p = F.pad(x, pad, mode='replicate')\n",
    "    return F.conv3d(x_p, kernel)\n",
    "\n",
    "def gaussian_blur3d(x, sigma: float):\n",
    "    # Gaussian smoothing wrapper\n",
    "    if sigma <= 0:\n",
    "        return x\n",
    "    kernel = _gaussian_kernel_3d(sigma, device=x.device, dtype=x.dtype)\n",
    "    return _conv3d_same(x, kernel)\n",
    "\n",
    "def _sobel_kernels_3d(device, dtype=torch.float32):\n",
    "    # 3D central-difference Sobel-like kernels\n",
    "    kx = torch.zeros((1,1,3,3,3), device=device, dtype=dtype)\n",
    "    ky = torch.zeros_like(kx)\n",
    "    kz = torch.zeros_like(kx)\n",
    "    kx[0,0,1,1,0] = -1.0; kx[0,0,1,1,2] = 1.0\n",
    "    ky[0,0,1,0,1] = -1.0; ky[0,0,1,2,1] = 1.0\n",
    "    kz[0,0,0,1,1] = -1.0; kz[0,0,2,1,1] = 1.0\n",
    "    return kx, ky, kz\n",
    "\n",
    "def gradient_magnitude_torch(x, sigma=0.0):\n",
    "    # Multi-scale gradient magnitude using optional Gaussian pre-blur\n",
    "    if sigma > 0:\n",
    "        x = gaussian_blur3d(x, sigma)\n",
    "    kx, ky, kz = _sobel_kernels_3d(x.device, dtype=x.dtype)\n",
    "    pad = _pad_for_conv(kx)\n",
    "    xpad = F.pad(x, pad, mode='replicate')\n",
    "    dx = F.conv3d(xpad, kx)\n",
    "    dy = F.conv3d(xpad, ky)\n",
    "    dz = F.conv3d(xpad, kz)\n",
    "    return torch.sqrt(dx*dx + dy*dy + dz*dz + 1e-12)\n",
    "\n",
    "def _global_quantile(tensor, q, max_samples=2_000_000):\n",
    "    \"\"\"\n",
    "    Compute approximate global quantile using a deterministic stride-subsample\n",
    "    when full flattening would exceed memory limits.\n",
    "    \"\"\"\n",
    "    flat = tensor.view(-1)\n",
    "    n = flat.numel()\n",
    "    if n <= max_samples:\n",
    "        return torch.quantile(flat, q)\n",
    "    step = int(n // max_samples) + 1\n",
    "    sample = flat[::step]\n",
    "    return torch.quantile(sample, q)\n",
    "\n",
    "def make_structural_anatomy_map(batch_imgs: torch.Tensor,\n",
    "                                grad_sigmas=(0.5, 2.0),\n",
    "                                hf_sigma=1.0,\n",
    "                                smooth_sigma=1.0,\n",
    "                                normalize_percentiles=(1.0, 99.0)):\n",
    "    \"\"\"\n",
    "    Crée une carte anatomique 3D unique (1 canal) de même dimension que l'entrée.\n",
    "    \"\"\"\n",
    "    assert batch_imgs.ndim == 5 and batch_imgs.shape[1] == 1\n",
    "    device, dtype = batch_imgs.device, batch_imgs.dtype\n",
    "\n",
    "    # Multi-scale gradient channels\n",
    "    g1 = gradient_magnitude_torch(batch_imgs, sigma=grad_sigmas[0])\n",
    "    g2 = gradient_magnitude_torch(batch_imgs, sigma=grad_sigmas[1])\n",
    "\n",
    "    # High-frequency residual |I - Gσ(I)|\n",
    "    blurred = gaussian_blur3d(batch_imgs, sigma=hf_sigma)\n",
    "    hf = torch.abs(batch_imgs - blurred)\n",
    "\n",
    "    # Weighted fusion\n",
    "    combined = 0.5 * g1 + 0.3 * g2 + 0.2 * hf\n",
    "\n",
    "    # Global robust normalization to [-1, 1]\n",
    "    p1, p99 = normalize_percentiles\n",
    "    lo = _global_quantile(combined, p1/100.0)\n",
    "    hi = _global_quantile(combined, p99/100.0)\n",
    "    normed = (combined - lo) / (hi - lo + 1e-6)\n",
    "    normed = normed.clamp(0, 1) * 2 - 1\n",
    "\n",
    "    # Optional anatomical smoothing\n",
    "    if smooth_sigma > 0:\n",
    "        normed = gaussian_blur3d(normed, sigma=smooth_sigma)\n",
    "\n",
    "    return normed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, step, model, embedder, optimizer, checkpoint_dir, accelerator):\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped = accelerator.unwrap_model(model)\n",
    "    if accelerator.is_main_process:\n",
    "        ckpt = {\n",
    "            \"model_state_dict\": unwrapped.state_dict(),\n",
    "            \"embedder_state_dict\": embedder.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"step\": step,\n",
    "        }\n",
    "        fname = os.path.join(checkpoint_dir, f\"ckpt_ep{epoch:04d}_crop_coarse.pt\")\n",
    "        torch.save(ckpt, fname)\n",
    "        print(f\"[checkpoint] saved -> {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint_if_exists(resume_from, model_diffusion, embedder, optimizer, accelerator):\n",
    "    \"\"\"\n",
    "    Load checkpoint if `resume_from` is provided and exists.\n",
    "    Must be called AFTER accelerator.prepare(...) because state_dicts are loaded\n",
    "    into the wrapped objects.\n",
    "    Returns:\n",
    "        start_epoch (int), global_step (int)\n",
    "    \"\"\"\n",
    "    if resume_from is None:\n",
    "        return 0, 0  # start at epoch 0, global_step 0\n",
    "\n",
    "    ckpt = torch.load(resume_from, map_location=accelerator.device)\n",
    "\n",
    "    # Must unwrap Accelerator-managed model before applying state_dict\n",
    "    unwrapped_model = accelerator.unwrap_model(model_diffusion)\n",
    "    unwrapped_model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "\n",
    "    embedder.load_state_dict(ckpt[\"embedder_state_dict\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "\n",
    "    start_epoch = ckpt.get(\"epoch\", 0)\n",
    "    global_step = ckpt.get(\"step\", 0)\n",
    "\n",
    "    print(f\"Resumed from checkpoint {resume_from} -> start_epoch={start_epoch}, global_step={global_step}\")\n",
    "\n",
    "    return start_epoch, global_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3_slices(volume_np):\n",
    "    # volume_np: 3D numpy array (D1,D2,D3)\n",
    "    d1, d2, d3 = volume_np.shape\n",
    "    i = d1 // 2\n",
    "    j = d2 // 2\n",
    "    k = d3 // 2\n",
    "    slice1 = volume_np[i, :, :]   # sagittal\n",
    "    slice2 = volume_np[:, j, :]   # coronal\n",
    "    slice3 = volume_np[:, :, k]   # axial\n",
    "    return slice1, slice2, slice3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop_3d_pair(batch_imgs, anatomic_cond, patch_size=(64, 64, 64)):\n",
    "    \"\"\"\n",
    "    Randomly extracts a 3D patch from both `batch_imgs` and `anatomic_cond`,\n",
    "    after performing an individual tight crop around the brain region\n",
    "    (mask != minimum value).\n",
    "\n",
    "    Args:\n",
    "        batch_imgs: Tensor of shape (B, 1, D, H, W)\n",
    "        anatomic_cond: Tensor of shape (B, 1, D, H, W)\n",
    "        patch_size: tuple (pd, ph, pw) defining patch depth, height, width\n",
    "\n",
    "    Returns:\n",
    "        patch_imgs: Tensor of shape (B, 1, pd, ph, pw)\n",
    "        patch_cond: Tensor of shape (B, 1, pd, ph, pw)\n",
    "    \"\"\"\n",
    "    assert batch_imgs.shape == anatomic_cond.shape\n",
    "    B, C, D, H, W = batch_imgs.shape\n",
    "    pd, ph, pw = patch_size\n",
    "\n",
    "    patch_imgs_list = []\n",
    "    patch_cond_list = []\n",
    "\n",
    "    for b in range(B):\n",
    "        # Brain mask: exclude background using min-intensity\n",
    "        mask3 = (batch_imgs[b, 0] != batch_imgs[b, 0].min())\n",
    "\n",
    "        # Extract bounding box of brain region\n",
    "        zs, ys, xs = torch.where(mask3)\n",
    "        zmin, zmax = int(zs.min().item()), int(zs.max().item())\n",
    "        ymin, ymax = int(ys.min().item()), int(ys.max().item())\n",
    "        xmin, xmax = int(xs.min().item()), int(xs.max().item())\n",
    "\n",
    "        # Tight crop (+1 for inclusive upper bound)\n",
    "        img_crop = batch_imgs[b:b+1, :, zmin:zmax+1, ymin:ymax+1, xmin:xmax+1]\n",
    "        cond_crop = anatomic_cond[b:b+1, :, zmin:zmax+1, ymin:ymax+1, xmin:xmax+1]\n",
    "\n",
    "        _, _, Dc, Hc, Wc = img_crop.shape\n",
    "\n",
    "        # Random valid patch origin in cropped volume\n",
    "        z0 = torch.randint(0, Dc - pd + 1, (1,)).item()\n",
    "        y0 = torch.randint(0, Hc - ph + 1, (1,)).item()\n",
    "        x0 = torch.randint(0, Wc - pw + 1, (1,)).item()\n",
    "\n",
    "        patch_img = img_crop[:, :, z0:z0+pd, y0:y0+ph, x0:x0+pw]\n",
    "        patch_cond = cond_crop[:, :, z0:z0+pd, y0:y0+ph, x0:x0+pw]\n",
    "\n",
    "        patch_imgs_list.append(patch_img)\n",
    "        patch_cond_list.append(patch_cond)\n",
    "\n",
    "    # Stack patches back into batch\n",
    "    patch_imgs = torch.cat(patch_imgs_list, dim=0)\n",
    "    patch_cond = torch.cat(patch_cond_list, dim=0)\n",
    "\n",
    "    return patch_imgs, patch_cond\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple, Sequence, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers: timestep embedding\n",
    "# ---------------------------\n",
    "def timestep_embedding(timesteps: torch.Tensor, dim: int, max_period: int = 10000):\n",
    "    \"\"\"\n",
    "    Sinusoidal timestep embedding, same style as common diffusion implementations.\n",
    "    timesteps: (B,) long\n",
    "    returns: (B, dim)\n",
    "    \"\"\"\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(\n",
    "        -math.log(max_period) * torch.arange(0, half, dtype=torch.float32, device=timesteps.device) / half\n",
    "    )\n",
    "    args = timesteps.float().unsqueeze(1) * freqs.unsqueeze(0)\n",
    "    emb = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "        emb = F.pad(emb, (0, 1))\n",
    "    return emb  # (B, dim)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Basic blocks in 3D\n",
    "# ---------------------------\n",
    "class Conv3dZeroInit(nn.Conv3d):\n",
    "    \"\"\"Conv3d with zero initialization option for residual projection (optionally).\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class ResidualBlock3D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim=None):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.GroupNorm(8, in_ch)\n",
    "        self.conv1 = nn.Conv3d(in_ch, out_ch, kernel_size=3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
    "        self.conv2 = nn.Conv3d(out_ch, out_ch, kernel_size=3, padding=1)\n",
    "        self.nin_shortcut = None\n",
    "        if in_ch != out_ch:\n",
    "            self.nin_shortcut = nn.Conv3d(in_ch, out_ch, kernel_size=1)\n",
    "        if time_emb_dim is not None:\n",
    "            self.time_mlp = nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(time_emb_dim, out_ch)\n",
    "            )\n",
    "        else:\n",
    "            self.time_mlp = None\n",
    "\n",
    "    def forward(self, x, t_emb=None):\n",
    "        h = self.norm1(x)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv1(h)\n",
    "        if self.time_mlp is not None and t_emb is not None:\n",
    "            # t_emb: (B, dim)\n",
    "            t = self.time_mlp(t_emb).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "            h = h + t\n",
    "        h = self.norm2(h)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv2(h)\n",
    "        if self.nin_shortcut is not None:\n",
    "            x = self.nin_shortcut(x)\n",
    "        return x + h\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Multi-head attention over 3D tokens (self-attention + cross-attention)\n",
    "# ---------------------------\n",
    "class MultiHeadAttention3D(nn.Module):\n",
    "    def __init__(self, dim, num_heads, head_dim, cross_dim=None):\n",
    "        \"\"\"\n",
    "        dim: input embedding dim\n",
    "        num_heads: number of heads\n",
    "        head_dim: dimension per head\n",
    "        cross_dim: if not None, cross-attention key/val come from vector of dim cross_dim\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.inner_dim = num_heads * head_dim\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.to_q = nn.Linear(dim, self.inner_dim, bias=False)\n",
    "        self.to_k = nn.Linear(dim if cross_dim is None else cross_dim, self.inner_dim, bias=False)\n",
    "        self.to_v = nn.Linear(dim if cross_dim is None else cross_dim, self.inner_dim, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(self.inner_dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        x: (B, N, dim) -- N = D*H*W tokens\n",
    "        context: if provided, keys/values come from context (B, M, cross_dim) -> cross-attention\n",
    "                 else use x (self-attention)\n",
    "        returns: (B, N, dim)\n",
    "        \"\"\"\n",
    "        b, n, _ = x.shape\n",
    "        context = x if context is None else context\n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(context)\n",
    "        v = self.to_v(context)\n",
    "        # reshape [B, N, heads, head_dim] -> [B, heads, N, head_dim]\n",
    "        q = q.view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(b, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(b, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # scaled dot-product\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # (B, heads, N, M)\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        out = torch.matmul(attn, v)  # (B, heads, N, head_dim)\n",
    "        out = out.transpose(1, 2).contiguous().view(b, n, self.inner_dim)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class AttentionBlock3D(nn.Module):\n",
    "    def __init__(self, channels, num_heads, head_dim, cross_attention_dim: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.norm = nn.GroupNorm(8, channels)\n",
    "        self.proj_in = nn.Conv3d(channels, channels, kernel_size=1)\n",
    "        self.proj_out = nn.Conv3d(channels, channels, kernel_size=1)\n",
    "        self.mha = MultiHeadAttention3D(dim=channels, num_heads=num_heads, head_dim=head_dim,\n",
    "                                        cross_dim=cross_attention_dim)\n",
    "\n",
    "        self.cross_attention_dim = cross_attention_dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor, context: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        x: (B, C, D, H, W)\n",
    "        context: (B, M, cross_attention_dim) or None\n",
    "        returns same shape as x\n",
    "        \"\"\"\n",
    "        b, c, d, h, w = x.shape\n",
    "        h_in = self.norm(x)\n",
    "        h_in = F.silu(h_in)\n",
    "        h_in = self.proj_in(h_in)  # (B, C, D, H, W)\n",
    "        # flatten spatial dims\n",
    "        h_flat = h_in.view(b, c, d * h * w).permute(0, 2, 1)  # (B, N, C)\n",
    "        if context is not None:\n",
    "            # context expected (B, M, cross_dim)\n",
    "            attn_out = self.mha(h_flat, context)  # (B, N, C)\n",
    "        else:\n",
    "            attn_out = self.mha(h_flat, None)\n",
    "        attn_out = attn_out.permute(0, 2, 1).view(b, c, d, h, w)\n",
    "        out = self.proj_out(attn_out)\n",
    "        return x + out\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Down / Up blocks\n",
    "# ---------------------------\n",
    "class Downsample3D(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.op = nn.Conv3d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.op(x)\n",
    "\n",
    "\n",
    "class Upsample3D(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.op = nn.ConvTranspose3d(channels, channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.op(x)\n",
    "\n",
    "\n",
    "class DownBlock3D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim=None, use_attn=False, num_heads=1, head_dim=32, cross_attention_dim=None):\n",
    "        super().__init__()\n",
    "        self.res1 = ResidualBlock3D(in_ch, out_ch, time_emb_dim=time_emb_dim)\n",
    "        self.attn = AttentionBlock3D(out_ch, num_heads, head_dim, cross_attention_dim) if use_attn else None\n",
    "        self.res2 = ResidualBlock3D(out_ch, out_ch, time_emb_dim=time_emb_dim)\n",
    "\n",
    "    def forward(self, x, t_emb=None, context=None):\n",
    "        x = self.res1(x, t_emb)\n",
    "        if self.attn is not None:\n",
    "            x = self.attn(x, context)\n",
    "        x = self.res2(x, t_emb)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpBlock3D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim=None, use_attn=False, num_heads=1, head_dim=32, cross_attention_dim=None):\n",
    "        super().__init__()\n",
    "        # in_ch is the concatenated channels from skip + current\n",
    "        self.res1 = ResidualBlock3D(in_ch, out_ch, time_emb_dim=time_emb_dim)\n",
    "        self.attn = AttentionBlock3D(out_ch, num_heads, head_dim, cross_attention_dim) if use_attn else None\n",
    "        self.res2 = ResidualBlock3D(out_ch, out_ch, time_emb_dim=time_emb_dim)\n",
    "\n",
    "    def forward(self, x, skip, t_emb=None, context=None):\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.res1(x, t_emb)\n",
    "        if self.attn is not None:\n",
    "            x = self.attn(x, context)\n",
    "        x = self.res2(x, t_emb)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# The main UNet3DConditionModel\n",
    "# ---------------------------\n",
    "class UNet3DConditionModel_maison(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_size: Tuple[int, int, int],\n",
    "        in_channels: int = 1,\n",
    "        out_channels: int = 1,\n",
    "        down_block_types: Sequence[str] = (\"DownBlock3D\", \"CrossAttnDownBlock3D\", \"CrossAttnDownBlock3D\", \"DownBlock3D\"),\n",
    "        up_block_types: Sequence[str] = (\"UpBlock3D\", \"CrossAttnUpBlock3D\", \"CrossAttnUpBlock3D\", \"UpBlock3D\"),\n",
    "        block_out_channels: Sequence[int] = (32, 64, 128, 256),\n",
    "        cross_attention_dim: int = 512,\n",
    "        attention_head_dim: int = 64,\n",
    "        time_embedding_dim: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(down_block_types) == len(up_block_types) == len(block_out_channels), \\\n",
    "            \"down_block_types, up_block_types and block_out_channels must have same length\"\n",
    "\n",
    "        self.sample_size = sample_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.block_out_channels = block_out_channels\n",
    "        self.cross_attention_dim = cross_attention_dim\n",
    "        self.attention_head_dim = attention_head_dim\n",
    "        self.time_embedding_dim = time_embedding_dim\n",
    "\n",
    "        # initial conv\n",
    "        self.conv_in = nn.Conv3d(in_channels, block_out_channels[0], kernel_size=3, padding=1)\n",
    "\n",
    "        # time embedding MLP\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(time_embedding_dim, time_embedding_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embedding_dim * 4, time_embedding_dim)\n",
    "        )\n",
    "\n",
    "        # build down blocks\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        self.downsamplers = nn.ModuleList()\n",
    "        prev_ch = block_out_channels[0]\n",
    "        for i, out_ch in enumerate(block_out_channels):\n",
    "            block_type = down_block_types[i]\n",
    "            use_attn = \"CrossAttn\" in block_type or \"Attn\" in block_type\n",
    "            num_heads = max(1, out_ch // attention_head_dim)\n",
    "            head_dim = attention_head_dim\n",
    "            db = DownBlock3D(prev_ch, out_ch, time_emb_dim=time_embedding_dim,\n",
    "                             use_attn=use_attn, num_heads=num_heads, head_dim=head_dim, cross_attention_dim=cross_attention_dim if use_attn else None)\n",
    "            self.down_blocks.append(db)\n",
    "            # add downsample except for last block\n",
    "            if i != len(block_out_channels) - 1:\n",
    "                self.downsamplers.append(Downsample3D(out_ch))\n",
    "            prev_ch = out_ch\n",
    "\n",
    "        # middle (bottleneck)\n",
    "        mid_ch = block_out_channels[-1]\n",
    "        self.mid_block1 = ResidualBlock3D(mid_ch, mid_ch, time_emb_dim=time_embedding_dim)\n",
    "        self.mid_attn = AttentionBlock3D(mid_ch, num_heads=max(1, mid_ch // attention_head_dim),\n",
    "                                         head_dim=attention_head_dim, cross_attention_dim=cross_attention_dim)\n",
    "        self.mid_block2 = ResidualBlock3D(mid_ch, mid_ch, time_emb_dim=time_embedding_dim)\n",
    "\n",
    "        # build up blocks\n",
    "        self.upsamplers = nn.ModuleList()\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        rev_out = list(reversed(block_out_channels))\n",
    "        prev_ch = rev_out[0]\n",
    "        for i, out_ch in enumerate(rev_out):\n",
    "            block_type = up_block_types[i]\n",
    "            use_attn = \"CrossAttn\" in block_type or \"Attn\" in block_type\n",
    "            num_heads = max(1, out_ch // attention_head_dim)\n",
    "            head_dim = attention_head_dim\n",
    "            # input channels for up block is prev_ch (current decoder) + skip channels (out_ch)\n",
    "            in_ch = prev_ch + out_ch\n",
    "            ub = UpBlock3D(in_ch, out_ch, time_emb_dim=time_embedding_dim,\n",
    "                           use_attn=use_attn, num_heads=num_heads, head_dim=head_dim, cross_attention_dim=cross_attention_dim if use_attn else None)\n",
    "            self.up_blocks.append(ub)\n",
    "            if i != len(rev_out) - 1:\n",
    "                self.upsamplers.append(Upsample3D(out_ch))\n",
    "            prev_ch = out_ch\n",
    "\n",
    "        # final normalization and conv\n",
    "        self.norm_out = nn.GroupNorm(8, block_out_channels[0])\n",
    "        self.conv_out = nn.Conv3d(block_out_channels[0], out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, sample: torch.Tensor, timestep: torch.Tensor, encoder_hidden_states: Optional[torch.Tensor] = None):\n",
    "        if timestep.dim() == 0:\n",
    "            timestep = timestep.view(1).expand(sample.shape[0])\n",
    "        t_emb = timestep_embedding(timestep, self.time_embedding_dim, max_period=10000)\n",
    "        t_emb = self.time_mlp(t_emb)\n",
    "\n",
    "        # input conv\n",
    "        x = self.conv_in(sample)\n",
    "        # print(\"After conv_in:\", x.shape)  # <-- dimension C après la première conv\n",
    "\n",
    "        # store skips\n",
    "        skips = []\n",
    "\n",
    "        # down path\n",
    "        for i, db in enumerate(self.down_blocks):\n",
    "            x = db(x, t_emb, encoder_hidden_states)\n",
    "            # print(f\"After down_block {i}:\", x.shape)  # <-- dimension C après chaque down_block\n",
    "            skips.append(x)\n",
    "            if i < len(self.downsamplers):\n",
    "                x = self.downsamplers[i](x)\n",
    "                # print(f\"After downsampler {i}:\", x.shape)  # <-- dimension C après chaque downsampler\n",
    "\n",
    "        # mid\n",
    "        x = self.mid_block1(x, t_emb)\n",
    "        # print(\"After mid_block1:\", x.shape)\n",
    "        x = self.mid_attn(x, encoder_hidden_states)\n",
    "        # print(\"After mid_attn:\", x.shape)\n",
    "        x = self.mid_block2(x, t_emb)\n",
    "        # print(\"After mid_block2:\", x.shape)\n",
    "\n",
    "        # up path\n",
    "        for i, ub in enumerate(self.up_blocks):\n",
    "            skip = skips.pop()\n",
    "            x = ub(x, skip, t_emb, encoder_hidden_states)\n",
    "            # print(f\"After up_block {i}:\", x.shape)  # <-- dimension C après chaque up_block\n",
    "            if i < len(self.upsamplers):\n",
    "                x = self.upsamplers[i](x)\n",
    "                # print(f\"After upsampler {i}:\", x.shape)  # <-- dimension C après chaque upsampler\n",
    "\n",
    "        x = self.norm_out(x)\n",
    "        # print(\"After norm_out:\", x.shape)\n",
    "        x = F.silu(x)\n",
    "        x = self.conv_out(x)\n",
    "        # print(\"After conv_out:\", x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "patch_size = (80,96,80)\n",
    "\n",
    "model_diffusion = UNet3DConditionModel_maison(\n",
    "    sample_size=patch_size,\n",
    "    in_channels=3,\n",
    "    out_channels=1,\n",
    "    down_block_types=(\"DownBlock3D\", \"CrossAttnDownBlock3D\", \"CrossAttnDownBlock3D\", \"DownBlock3D\"),\n",
    "    up_block_types=(\"UpBlock3D\", \"CrossAttnUpBlock3D\", \"CrossAttnUpBlock3D\", \"UpBlock3D\"),\n",
    "    block_out_channels=(64, 128, 256, 256),\n",
    "    cross_attention_dim=512,\n",
    "    attention_head_dim=64,\n",
    "    time_embedding_dim=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- HYPERPARAMS ----------\n",
    "num_epochs = 3000\n",
    "lr = 1e-4\n",
    "grad_accum_steps = 1  \n",
    "eval_every_epoch = 10\n",
    "save_every_epoch = 10\n",
    "checkpoint_dir = \"folder_to_save/checkpoint\"\n",
    "resume_from = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(mixed_precision=\"fp16\")  # enable FP16 training for speed/memory\n",
    "\n",
    "n_classes = len(all_labels)\n",
    "noise_scheduler = DDIMScheduler(num_train_timesteps=1000)\n",
    "embedder = nn.Embedding(n_classes + 1, 512)  # class-conditioning embedding\n",
    "optimizer = Adam(\n",
    "    list(model_diffusion.parameters()) + list(embedder.parameters()),\n",
    "    lr=lr,\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Wrap all components for distributed / mixed-precision execution\n",
    "model_diffusion, embedder, optimizer, train_loader, test_loader = accelerator.prepare(\n",
    "    model_diffusion, embedder, optimizer, train_loader, test_loader\n",
    ")\n",
    "\n",
    "# Initialize DDIM sampling steps\n",
    "noise_scheduler.set_timesteps(noise_scheduler.num_train_timesteps)\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# Load checkpoint after accelerator.prepare (important for wrapped models)\n",
    "start_epoch, global_step = load_checkpoint_if_exists(\n",
    "    resume_from, model_diffusion, embedder, optimizer, accelerator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger TensorBoard (Main process only)\n",
    "# -------------------------\n",
    "writer = None\n",
    "if accelerator.is_main_process:\n",
    "    tb_log_dir = \"path_to/log_dir\"\n",
    "    os.makedirs(tb_log_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(tb_log_dir)\n",
    "    print(f\"[logger] TensorBoard writer created at {tb_log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# TRAINING\n",
    "# -------------------------\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model_diffusion.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader),\n",
    "                        desc=f\"Epoch {epoch}\", ncols=120)\n",
    "\n",
    "    for step_in_epoch, batch in progress_bar:\n",
    "\n",
    "        global_step += 1\n",
    "        volumes, labels = batch\n",
    "        volumes = volumes.to(accelerator.device, non_blocking=True).float()\n",
    "\n",
    "        # Gamma transformation to prevent anatomical map from encoding gamma information\n",
    "        volumes_transform = volumes.clone()\n",
    "        volumes_transform_gamma = torch.stack([\n",
    "            tio.RandomGamma(log_gamma=(-0.5, 0.5), p=1.0)(\n",
    "                tio.ScalarImage(tensor=img)\n",
    "            ).data\n",
    "            for img in volumes_transform\n",
    "        ])\n",
    "\n",
    "        batch_size = volumes.shape[0]\n",
    "\n",
    "        # Random sigma jittering for anatomical map\n",
    "        g1_sigma = random.uniform(0.1, 0.7)\n",
    "        g2_sigma = random.uniform(0.7, 1.4)\n",
    "        hf_sigma = random.uniform(0.1, 1.0)\n",
    "\n",
    "        volume_anat_map = make_structural_anatomy_map(\n",
    "            volumes_transform_gamma,\n",
    "            grad_sigmas=(g1_sigma, g2_sigma),\n",
    "            hf_sigma=hf_sigma,\n",
    "            smooth_sigma=0.0,\n",
    "            normalize_percentiles=(0.5, 99.5)\n",
    "        )\n",
    "\n",
    "        anat_coarse = F.interpolate(\n",
    "            volume_anat_map.float(),\n",
    "            size=patch_size,\n",
    "            mode='trilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "\n",
    "        # Paired anatomical + image patch extraction\n",
    "        patch_volume, patch_anat_map = random_crop_3d_pair(\n",
    "            volumes, volume_anat_map, patch_size=patch_size\n",
    "        )\n",
    "\n",
    "        timesteps = torch.randint(\n",
    "            0,\n",
    "            noise_scheduler.num_train_timesteps,\n",
    "            (batch_size,),\n",
    "            device=accelerator.device,\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "\n",
    "        noise = torch.randn_like(patch_volume)\n",
    "        noisy_latents = noise_scheduler.add_noise(patch_volume, noise, timesteps)\n",
    "\n",
    "        # Class-conditioning (with classifier-free masking)\n",
    "        label_ids = torch.tensor(\n",
    "            [ds2id[l] for l in labels],\n",
    "            device=accelerator.device,\n",
    "            dtype=torch.long\n",
    "        )\n",
    "\n",
    "        p_uncond = 0.15\n",
    "        mask = torch.rand(label_ids.shape, device=accelerator.device) < p_uncond\n",
    "        label_ids_masked = label_ids.clone()\n",
    "        if mask.any():\n",
    "            label_ids_masked[mask] = 0\n",
    "\n",
    "        label_embedding = embedder(label_ids_masked)\n",
    "        label_embedding = label_embedding.unsqueeze(1)  # (B, 1, embed_dim)\n",
    "\n",
    "        # Model input: noisy patch + fine anatomy + coarse anatomy\n",
    "        model_input = torch.cat([noisy_latents, patch_anat_map, anat_coarse], dim=1)\n",
    "        model_output = model_diffusion(\n",
    "            model_input, timesteps, encoder_hidden_states=label_embedding\n",
    "        )\n",
    "\n",
    "        noise_pred = model_output\n",
    "        loss = mse_loss(noise_pred.float(), noise.float())\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        if accelerator.sync_gradients:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        avg_loss = epoch_loss / float(step_in_epoch + 1)\n",
    "        progress_bar.set_postfix({\"loss\": f\"{avg_loss:.6f}\"})\n",
    "\n",
    "    # TensorBoard logging (main process only)\n",
    "    if accelerator.is_main_process and writer is not None:\n",
    "        writer.add_scalar(\"train/loss_epoch\", epoch_loss / len(train_loader), epoch)\n",
    "\n",
    "    # -------------------------\n",
    "    # EVALUATION\n",
    "    # -------------------------\n",
    "\n",
    "    if (epoch + 1) % eval_every_epoch == 0 or epoch == start_epoch:\n",
    "        model_diffusion.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Take a single batch for eval\n",
    "            for eval_batch in test_loader:\n",
    "                eval_volumes, eval_labels = eval_batch\n",
    "                eval_volumes = eval_volumes.to(accelerator.device, non_blocking=True).float()\n",
    "                break\n",
    "\n",
    "            # Build eval anatomical maps\n",
    "            eval_volume_anat_map = make_structural_anatomy_map(\n",
    "                eval_volumes,\n",
    "                grad_sigmas=(0.3, 1.0),\n",
    "                hf_sigma=1.0,\n",
    "                smooth_sigma=0.0,\n",
    "                normalize_percentiles=(0.5, 99.5)\n",
    "            )\n",
    "\n",
    "            eval_anat_coarse = F.interpolate(\n",
    "                eval_volume_anat_map.float(),\n",
    "                size=patch_size,\n",
    "                mode='nearest'\n",
    "            )\n",
    "\n",
    "            patch_volume, patch_anat_map = random_crop_3d_pair(\n",
    "                eval_volumes, eval_volume_anat_map, patch_size=patch_size\n",
    "            )\n",
    "\n",
    "            # Embeddings for conditional & unconditional sampling\n",
    "            label_ids_eval = torch.tensor(\n",
    "                [ds2id[l] for l in eval_labels],\n",
    "                device=accelerator.device,\n",
    "                dtype=torch.long\n",
    "            )\n",
    "            cond_label_embedding = embedder(label_ids_eval).unsqueeze(1)\n",
    "\n",
    "            uncond_ids = torch.zeros_like(label_ids_eval, dtype=torch.long,\n",
    "                                          device=label_ids_eval.device)\n",
    "            uncond_label_embedding = embedder(uncond_ids).unsqueeze(1)\n",
    "\n",
    "            batch_size_eval = eval_volumes.shape[0]\n",
    "            num_inference_steps = 50\n",
    "            noise_scheduler.set_timesteps(num_inference_steps)\n",
    "            timesteps_iter = list(noise_scheduler.timesteps)\n",
    "\n",
    "            gaussian_noise = torch.randn_like(patch_volume).to(accelerator.device)\n",
    "\n",
    "            num_train_timesteps = noise_scheduler.config.num_train_timesteps\n",
    "            num_inference_steps = noise_scheduler.num_inference_steps\n",
    "            step_offset = num_train_timesteps // num_inference_steps\n",
    "\n",
    "            alphas_cumprod = noise_scheduler.alphas_cumprod.to(accelerator.device)\n",
    "            final_alpha_cumprod = noise_scheduler.final_alpha_cumprod.to(accelerator.device)\n",
    "\n",
    "            # DDIM-like inference loop\n",
    "            for t in timesteps_iter:\n",
    "                t_b = torch.tensor(\n",
    "                    [int(t)] * batch_size_eval,\n",
    "                    device=accelerator.device,\n",
    "                    dtype=torch.long\n",
    "                )\n",
    "\n",
    "                # Duplicate input for cond/uncond\n",
    "                volume_in = torch.cat([gaussian_noise, gaussian_noise], dim=0)\n",
    "                anatomy_in = torch.cat([patch_anat_map, patch_anat_map], dim=0)\n",
    "                coarse_in = torch.cat([eval_anat_coarse, eval_anat_coarse], dim=0)\n",
    "\n",
    "                model_input_eval = torch.cat([volume_in, anatomy_in, coarse_in], dim=1)\n",
    "\n",
    "                emb_in = torch.cat([uncond_label_embedding, cond_label_embedding], dim=0)\n",
    "                t_in = torch.cat([t_b, t_b], dim=0)\n",
    "\n",
    "                # Model prediction\n",
    "                model_output_eval = model_diffusion(\n",
    "                    model_input_eval, t_in, encoder_hidden_states=emb_in\n",
    "                )\n",
    "\n",
    "                uncond_noise_pred, conde_noise_pred = model_output_eval.chunk(2, dim=0)\n",
    "\n",
    "                # Classifier-free guidance\n",
    "                prev_t = t - step_offset\n",
    "                alpha_t = alphas_cumprod[t]\n",
    "                alpha_prev = alphas_cumprod[prev_t] if prev_t >= 0 else final_alpha_cumprod\n",
    "                beta_t = 1 - alpha_t\n",
    "\n",
    "                guidance_scale = 1.0\n",
    "                pred = uncond_noise_pred + guidance_scale * (conde_noise_pred - uncond_noise_pred)\n",
    "\n",
    "                # DDIM prediction of x0\n",
    "                x0_pred = (gaussian_noise - beta_t**0.5 * pred) / alpha_t**0.5\n",
    "\n",
    "                coeff_dir = (1 - alpha_prev)**0.5\n",
    "                pred_sample_direction = coeff_dir * uncond_noise_pred\n",
    "\n",
    "                # Sample at next timestep\n",
    "                gaussian_noise = alpha_prev**0.5 * x0_pred + pred_sample_direction\n",
    "\n",
    "            diffused_latents = gaussian_noise\n",
    "\n",
    "            # -------------------------\n",
    "            # VISUALIZATION\n",
    "            # -------------------------\n",
    "            patch_volume_np = patch_volume[0,0].cpu().numpy()\n",
    "            patch_anat_map_np = patch_anat_map[0,0].cpu().numpy()\n",
    "            diffused_latents_np = diffused_latents[0,0].cpu().numpy()\n",
    "\n",
    "            volume_s1, volume_s2, volume_s3 = get_3_slices(patch_volume_np)\n",
    "            anat_s1, anat_s2, anat_s3 = get_3_slices(patch_anat_map_np)\n",
    "            gen_recon_s1, gen_recon_s2, gen_recon_s3 = get_3_slices(diffused_latents_np)\n",
    "\n",
    "            fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "            plt.subplots_adjust(wspace=0.05, hspace=0.2)\n",
    "\n",
    "            row_titles = [\n",
    "                \"Original (3 slices)\",\n",
    "                \"Anat map (3 slices)\",\n",
    "                \"Reconstruction (3 slices)\",\n",
    "            ]\n",
    "\n",
    "            slice_sets = [\n",
    "                (volume_s1, volume_s2, volume_s3),\n",
    "                (anat_s1, anat_s2, anat_s3),\n",
    "                (gen_recon_s1, gen_recon_s2, gen_recon_s3)\n",
    "            ]\n",
    "\n",
    "            for r in range(3):\n",
    "                s1, s2, s3 = slice_sets[r]\n",
    "                for c, im in enumerate([s1, s2, s3]):\n",
    "                    ax = axes[r, c]\n",
    "                    im_show = np.rot90(im)\n",
    "\n",
    "                    # Dynamic contrast normalization\n",
    "                    p1, p99 = np.percentile(im_show, [1, 99])\n",
    "                    if p1 == p99:\n",
    "                        ax.imshow(im_show, cmap=\"gray\")\n",
    "                    else:\n",
    "                        im_clipped = np.clip(im_show, p1, p99)\n",
    "                        im_norm = (im_clipped - p1) / (p99 - p1 + 1e-5)\n",
    "                        ax.imshow(im_norm, cmap=\"gray\", vmin=0, vmax=1)\n",
    "\n",
    "                    ax.axis(\"off\")\n",
    "                    if c == 1:\n",
    "                        ax.set_title(row_titles[r], fontsize=10)\n",
    "\n",
    "            vis_fname = os.path.join(checkpoint_dir, f\"vis_epoch{epoch:04d}_full_inference.png\")\n",
    "\n",
    "            if accelerator.is_main_process:\n",
    "                if writer is not None:\n",
    "                    writer.add_figure(\"eval/full_inference_visualization\", fig, epoch)\n",
    "            plt.close(fig)\n",
    "\n",
    "    # -------------------------\n",
    "    # CHECKPOINTING\n",
    "    # -------------------------\n",
    "    if (epoch + 1) % save_every_epoch == 0 or (epoch == num_epochs - 1):\n",
    "        save_checkpoint(\n",
    "            epoch + 1,\n",
    "            global_step,\n",
    "            model_diffusion,\n",
    "            embedder,\n",
    "            optimizer,\n",
    "            checkpoint_dir,\n",
    "            accelerator\n",
    "        )\n",
    "\n",
    "if accelerator.is_main_process and writer is not None:\n",
    "    writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "barnabe_models_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
